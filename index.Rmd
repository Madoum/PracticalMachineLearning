---
title: "Practical Machine Learning Course Project"
author: "MABE TENE EPSE FONGANG IVETTE"
date: "17 April 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## INSTRUCTIONS

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.

## INPUT DATA

```{r}
ExoData=read.csv(file="pml-training.csv", na.strings=c("NA",""), header=TRUE)
nrow(ExoData)
str(ExoData)
dim(ExoData)
```
## FEATURES
Having verified that the schema of both the training and testing sets are not identical, I decided to eliminate both NA columns and other extraneous columns.

```{r}
b=sapply(ExoData, function(x) sum(is.na(x)))
FullData=subset(ExoData,select=c(which(!b>0)), stringsAsFactors=FALSE)
str(FullData)
dim(FullData)
```

## RANDOM FOREST

```{r}
library(caret)
library(mlbench)
```

## CROSS VALDATION


 I decided to assess the impact/value of including preprocessing.
```{r}
tData=smartData
tData$cvtd_timestamp=NULL #with these in, there was a factor level mismatch with final validation set
tData$new_window=NULL #  final validation set
dim(tData)
trainIdx=createDataPartition(tData$classe, p = .75, list=FALSE)
trainD=tData[trainIdx,]
testD=tData[-trainIdx,]
x <- trainD[,-58]
y <- trainD[,58]

```

```{r}
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
```

```{r}
fitControl <- trainControl(method = "cv",
                           number = 10,
                           allowParallel = TRUE)
```

```{r}
fit <- train(x,y, method="rf",data=trainD,trControl = fitControl)
plot(fit)
```
The confusion matrix show a very high accuracy in the random forest model, therefore this model will be selected to predict the values in the test set
Predicting new values

```{r}
rf.pred=predict(fit,testD[,-60])
confusionMat=table(rf.pred,testD[,60])
confusionMat
```

```{r}
#str(trainD)
```
The results appear to be too precise to be true, which means we are probably overfitting. However, we are using cross validation to select the variables and it appears peak above 0.9998 accuracy around 30 variables. We where unable to find an explanation for overfitting and had to conclude that the high performance on the test set is probably due to data being artificial not having enough noise.
```{r}
vData=read.csv(file="pml-testing.csv", na.strings=c("NA",""), header=TRUE)
b=sapply(vData, function(x) sum(is.na(x)))
fvData=subset(vData,select=c(which(!b>0)), stringsAsFactors=FALSE)
#str(vData)
#dim(vData)
svData=fvData
svData=svData[,colSums(svData != 0) != 0] 
dim(svData)
s=sapply(svData, function(x) sum(is.na(x)))
str(svData)
```

Finally we use the cm_rf model in the test set to predict the “Classe” variable
```{r}
validationData=svData
validationData$cvtd_timestamp=NULL
validationData$new_window=NULL
nrow(validationData)
str(validationData)
dim(validationData)
validation.pred=predict(fit, validationData)
validation.pred
```

  